# 모델 기본 설정
name: "facebook/bart-base"
tokenizer_name: "facebook/bart-base"
max_length: 128
num_beams: 4
learning_rate: 5e-5
weight_decay: 0.01
use_fp16: false
use_pretrained: true

# Fine-tuning 전략
fine_tuning:
  unfreeze_layers:
    - "encoder.layers.[0-5]"
    - "decoder.layers.[0-5]"
    - "shared"
    - "layernorm"
    - "final_layer_norm"
  gradient_checkpointing: true
  freeze_embeddings: true
  quantization: "none"  # none, 8bit, qlora
  lora_config:
    enabled: false
    rank: 8
    alpha: 32
    dropout: 0.1
  learning_rate: 1e-4

# 생성 파라미터
generation:
  max_length: 150
  min_length: 50
  length_penalty: 2.0
  no_repeat_ngram_size: 3
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.2

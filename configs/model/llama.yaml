name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer_name: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

max_length: 512
num_beams: 4
learning_rate: 3e-5
weight_decay: 0.01

# Fine-tuning 설정 추가
finetune_strategy:
  unfreeze_layers:
    - "model.layers.31"  # 마지막 레이어
    - "model.layers.30"  # 마지막에서 두 번째 레이어
    - "lm_head"         # 출력 레이어
  gradient_checkpointing: true
  freeze_embeddings: true

name: "t5-base"
tokenizer_name: "t5-base"
max_length: 128
num_beams: 4
learning_rate: 5e-5
weight_decay: 0.01
use_pretrained: true

# Fine-tuning 전략
fine_tuning:
  unfreeze_layers:
    - "encoder.block.11"
    - "encoder.block.10"
    - "decoder.block.11"
    - "decoder.block.10"
    - "shared"
  gradient_checkpointing: true
  freeze_embeddings: true
  quantization: "none"
  lora_config:
    enabled: false
    rank: 8
    alpha: 32
    dropout: 0.1
    target_modules:
      t5: ["q", "v"]

# 생성 파라미터
generation:
  max_length: 150
  min_length: 50
  length_penalty: 2.0
  no_repeat_ngram_size: 3
  temperature: 0.7
  top_p: 0.9
  repetition_penalty: 1.2
